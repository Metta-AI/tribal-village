"""Self-documenting configuration with Pydantic validation.

This module provides typed, validated configuration classes for the Tribal Village
environment. Based on the mettascope pattern with type hints, field descriptors,
validation constraints, auto-generated help/docs, and deterministic serialization.

Usage:
    # Create environment config
    env_config = EnvironmentConfig(max_steps=5000, render_mode="ansi")

    # Override nested values using dot notation
    env_config.override("rewards.heart", 1.5)

    # Convert to dict for compatibility
    config_dict = env_config.model_dump()

    # Serialize for reproducibility
    config_json = env_config.model_dump_json()
"""

from __future__ import annotations

import math
from typing import Any, ClassVar, NoReturn, Optional, Self, Union, get_args, get_origin

from pydantic import BaseModel, ConfigDict, Field, TypeAdapter, field_validator


class Config(BaseModel):
    """Base configuration class with override support and validation.

    This class extends Pydantic's BaseModel to provide:
    - Strict field validation (extra="forbid")
    - Dot-notation path overrides (config.override("nested.field", value))
    - Batch updates (config.update({"a.b": 1, "c.d": 2}))
    - Auto-initialization of None Config fields
    """

    model_config: ClassVar[ConfigDict] = ConfigDict(extra="forbid")

    def _auto_initialize_field(self, parent_obj: Config, field_name: str) -> Config | None:
        """Auto-initialize a None Config field if possible."""
        field = type(parent_obj).model_fields.get(field_name)
        if not field:
            return None

        field_type = self._unwrap_optional(field.annotation)
        if not (isinstance(field_type, type) and issubclass(field_type, Config)):
            return None

        try:
            new_instance = field_type()
            setattr(parent_obj, field_name, new_instance)
            return new_instance
        except (TypeError, ValueError):
            return None

    def _unwrap_optional(self, field_type: Any) -> Any:
        """Unwrap Optional[T] to T if applicable, else return original type."""
        if get_origin(field_type) is Union:
            non_none_types = [arg for arg in get_args(field_type) if arg is not type(None)]
            return non_none_types[0] if len(non_none_types) == 1 else field_type
        return field_type

    def override(self, key: str, value: Any) -> Self:
        """Override a value in the config using dot-notation path.

        Examples:
            config.override("max_steps", 1000)
            config.override("rewards.heart", 1.5)
            config.override("ppo.learning_rate", 0.0001)
        """
        key_path = key.split(".")

        def fail(error: str) -> NoReturn:
            raise ValueError(
                f"Override failed. Full config:\n{self.model_dump_json(indent=2)}\n"
                f"Override {key} failed: {error}"
            )

        inner_cfg: Config | dict[str, Any] = self
        traversed_path: list[str] = []
        i = 0

        while i < len(key_path) - 1:
            key_part = key_path[i]

            if isinstance(inner_cfg, dict):
                if key_part in inner_cfg:
                    inner_cfg = inner_cfg[key_part]
                    traversed_path.append(key_part)
                    i += 1
                    continue

                remaining_path = ".".join(key_path[i:])
                if remaining_path in inner_cfg:
                    inner_cfg[remaining_path] = value
                    return self

                if i == len(key_path) - 2:
                    inner_cfg[remaining_path] = value
                    return self

                fail(f"key {key} not found in dict at path {'.'.join(traversed_path)}")

            if not hasattr(inner_cfg, key_part):
                failed_path = ".".join(traversed_path + [key_part])
                fail(f"key {failed_path} not found")

            next_inner_cfg = getattr(inner_cfg, key_part)
            if next_inner_cfg is None:
                next_inner_cfg = self._auto_initialize_field(inner_cfg, key_part)
                if next_inner_cfg is None:
                    failed_path = ".".join(traversed_path + [key_part])
                    fail(f"Cannot auto-initialize None field {failed_path}")

            if not isinstance(next_inner_cfg, (Config, dict)):
                failed_path = ".".join(traversed_path + [key_part])
                fail(f"key {failed_path} is not a Config object")

            inner_cfg = next_inner_cfg
            traversed_path.append(key_part)
            i += 1

        if isinstance(inner_cfg, Config):
            if not hasattr(inner_cfg, key_path[-1]):
                fail(f"key {key} not found")

        if isinstance(inner_cfg, dict):
            final_key = key_path[-1]
            if final_key in inner_cfg:
                inner_cfg[final_key] = value
            else:
                remaining = ".".join(key_path[i:])
                if remaining in inner_cfg:
                    inner_cfg[remaining] = value
                else:
                    inner_cfg[final_key] = value
            return self

        cls = type(inner_cfg)
        field = cls.model_fields.get(key_path[-1])
        if field is None:
            fail(f"key {key} is not a valid field")

        value = TypeAdapter(field.annotation).validate_python(value)
        setattr(inner_cfg, key_path[-1], value)

        return self

    def update(self, updates: dict[str, Any]) -> Self:
        """Apply multiple overrides to the config."""
        for key, value in updates.items():
            self.override(key, value)
        return self


class RewardConfig(Config):
    """Configuration for reward parameters.

    Rewards are applied when the corresponding events occur in the game.
    Penalties are negative rewards applied on certain conditions.
    Use math.nan for rewards that should be inherited from defaults.
    """

    heart: float = Field(
        default=math.nan,
        description="Reward for collecting a heart (healing item)",
    )
    ore: float = Field(
        default=math.nan,
        description="Reward for collecting ore resource",
    )
    bar: float = Field(
        default=math.nan,
        description="Reward for creating a metal bar from ore",
    )
    wood: float = Field(
        default=math.nan,
        description="Reward for collecting wood resource",
    )
    water: float = Field(
        default=math.nan,
        description="Reward for collecting water resource",
    )
    wheat: float = Field(
        default=math.nan,
        description="Reward for harvesting wheat",
    )
    spear: float = Field(
        default=math.nan,
        description="Reward for crafting a spear weapon",
    )
    armor: float = Field(
        default=math.nan,
        description="Reward for crafting armor",
    )
    food: float = Field(
        default=math.nan,
        description="Reward for producing food",
    )
    cloth: float = Field(
        default=math.nan,
        description="Reward for producing cloth",
    )
    tumor_kill: float = Field(
        default=math.nan,
        description="Reward for killing a tumor enemy",
    )
    survival_penalty: float = Field(
        default=math.nan,
        description="Penalty applied each step for agent survival",
    )
    death_penalty: float = Field(
        default=math.nan,
        description="Penalty applied when an agent dies",
    )


class EnvironmentConfig(Config):
    """Configuration for the Tribal Village environment.

    This configuration controls the simulation parameters, rendering,
    and reward structure for the environment.
    """

    # Core simulation parameters
    max_steps: int = Field(
        default=10_000,
        ge=1,
        description="Maximum number of simulation steps per episode",
    )
    victory_condition: int = Field(
        default=0,
        ge=0,
        description="Victory condition type (0=survival, others TBD)",
    )
    tumor_spawn_rate: float = Field(
        default=math.nan,
        description="Rate at which tumor enemies spawn (per step probability)",
    )

    # Rendering parameters
    render_mode: str = Field(
        default="rgb_array",
        description="Render mode: 'rgb_array', 'ansi', or 'human'",
    )
    render_scale: int = Field(
        default=4,
        ge=1,
        description="Scale factor for rendered output",
    )
    ansi_buffer_size: int = Field(
        default=1_000_000,
        ge=1000,
        description="Buffer size for ANSI rendering output",
    )

    # Reward configuration
    rewards: RewardConfig = Field(
        default_factory=RewardConfig,
        description="Reward parameters for various game events",
    )

    @field_validator("render_mode")
    @classmethod
    def validate_render_mode(cls, v: str) -> str:
        valid_modes = {"rgb_array", "ansi", "human"}
        if v not in valid_modes:
            raise ValueError(f"render_mode must be one of {valid_modes}, got '{v}'")
        return v

    def to_legacy_dict(self) -> dict[str, Any]:
        """Convert to legacy dictionary format for backward compatibility."""
        result: dict[str, Any] = {
            "max_steps": self.max_steps,
            "victory_condition": self.victory_condition,
            "render_mode": self.render_mode,
            "render_scale": self.render_scale,
        }

        # Add reward fields with legacy naming
        if not math.isnan(self.tumor_spawn_rate):
            result["tumor_spawn_rate"] = self.tumor_spawn_rate
        if not math.isnan(self.rewards.heart):
            result["heart_reward"] = self.rewards.heart
        if not math.isnan(self.rewards.ore):
            result["ore_reward"] = self.rewards.ore
        if not math.isnan(self.rewards.bar):
            result["bar_reward"] = self.rewards.bar
        if not math.isnan(self.rewards.wood):
            result["wood_reward"] = self.rewards.wood
        if not math.isnan(self.rewards.water):
            result["water_reward"] = self.rewards.water
        if not math.isnan(self.rewards.wheat):
            result["wheat_reward"] = self.rewards.wheat
        if not math.isnan(self.rewards.spear):
            result["spear_reward"] = self.rewards.spear
        if not math.isnan(self.rewards.armor):
            result["armor_reward"] = self.rewards.armor
        if not math.isnan(self.rewards.food):
            result["food_reward"] = self.rewards.food
        if not math.isnan(self.rewards.cloth):
            result["cloth_reward"] = self.rewards.cloth
        if not math.isnan(self.rewards.tumor_kill):
            result["tumor_kill_reward"] = self.rewards.tumor_kill
        if not math.isnan(self.rewards.survival_penalty):
            result["survival_penalty"] = self.rewards.survival_penalty
        if not math.isnan(self.rewards.death_penalty):
            result["death_penalty"] = self.rewards.death_penalty

        return result

    @classmethod
    def from_legacy_dict(cls, config: dict[str, Any]) -> EnvironmentConfig:
        """Create config from legacy dictionary format."""
        rewards = RewardConfig(
            heart=config.get("heart_reward", math.nan),
            ore=config.get("ore_reward", math.nan),
            bar=config.get("bar_reward", math.nan),
            wood=config.get("wood_reward", math.nan),
            water=config.get("water_reward", math.nan),
            wheat=config.get("wheat_reward", math.nan),
            spear=config.get("spear_reward", math.nan),
            armor=config.get("armor_reward", math.nan),
            food=config.get("food_reward", math.nan),
            cloth=config.get("cloth_reward", math.nan),
            tumor_kill=config.get("tumor_kill_reward", math.nan),
            survival_penalty=config.get("survival_penalty", math.nan),
            death_penalty=config.get("death_penalty", math.nan),
        )

        return cls(
            max_steps=config.get("max_steps", 10_000),
            victory_condition=config.get("victory_condition", 0),
            tumor_spawn_rate=config.get("tumor_spawn_rate", math.nan),
            render_mode=config.get("render_mode", "rgb_array"),
            render_scale=config.get("render_scale", 4),
            ansi_buffer_size=config.get("ansi_buffer_size", 1_000_000),
            rewards=rewards,
        )


class PPOConfig(Config):
    """Configuration for PPO (Proximal Policy Optimization) hyperparameters.

    These parameters control the PPO training algorithm behavior.
    """

    learning_rate: float = Field(
        default=0.0005,
        gt=0,
        description="Learning rate for the optimizer",
    )
    bptt_horizon: int = Field(
        default=64,
        ge=1,
        description="Backpropagation through time horizon for RNN training",
    )
    adam_eps: float = Field(
        default=1e-8,
        gt=0,
        description="Epsilon for Adam optimizer numerical stability",
    )
    adam_beta1: float = Field(
        default=0.95,
        ge=0,
        le=1,
        description="Adam optimizer beta1 (momentum)",
    )
    adam_beta2: float = Field(
        default=0.999,
        ge=0,
        le=1,
        description="Adam optimizer beta2 (RMSprop)",
    )
    gamma: float = Field(
        default=0.995,
        ge=0,
        le=1,
        description="Discount factor for future rewards",
    )
    gae_lambda: float = Field(
        default=0.90,
        ge=0,
        le=1,
        description="Lambda for Generalized Advantage Estimation",
    )
    update_epochs: int = Field(
        default=1,
        ge=1,
        description="Number of epochs per PPO update",
    )
    clip_coef: float = Field(
        default=0.2,
        ge=0,
        description="PPO clipping coefficient",
    )
    vf_coef: float = Field(
        default=2.0,
        ge=0,
        description="Value function loss coefficient",
    )
    vf_clip_coef: float = Field(
        default=0.2,
        ge=0,
        description="Value function clipping coefficient",
    )
    max_grad_norm: float = Field(
        default=1.5,
        gt=0,
        description="Maximum gradient norm for clipping",
    )
    ent_coef: float = Field(
        default=0.01,
        ge=0,
        description="Entropy coefficient for exploration",
    )
    max_minibatch_size: int = Field(
        default=32768,
        ge=1,
        description="Maximum minibatch size",
    )
    vtrace_rho_clip: float = Field(
        default=1.0,
        ge=0,
        description="V-trace rho clipping coefficient",
    )
    vtrace_c_clip: float = Field(
        default=1.0,
        ge=0,
        description="V-trace c clipping coefficient",
    )
    prio_alpha: float = Field(
        default=0.8,
        ge=0,
        le=1,
        description="Priority exponent for prioritized replay",
    )
    prio_beta0: float = Field(
        default=0.2,
        ge=0,
        le=1,
        description="Initial importance sampling exponent",
    )


class PolicyConfig(Config):
    """Configuration for policy network architecture."""

    hidden_size: int = Field(
        default=256,
        ge=1,
        description="Hidden layer size for policy network",
    )
    class_path: Optional[str] = Field(
        default=None,
        description="Full class path for custom policy implementation",
    )


class TrainingConfig(Config):
    """Configuration for training loop parameters.

    This configuration controls the training process including
    environment count, batch sizes, and checkpoint intervals.
    """

    # Environment parameters
    max_steps: int = Field(
        default=1_000,
        ge=1,
        description="Maximum training steps per episode",
    )
    num_envs: int = Field(
        default=64,
        ge=1,
        description="Number of parallel environments",
    )
    checkpoint_interval: int = Field(
        default=200,
        ge=1,
        description="Steps between checkpoints",
    )

    # Batch parameters
    batch_size: Optional[int] = Field(
        default=None,
        ge=1,
        description="Batch size for training (None = auto)",
    )
    minibatch_size: Optional[int] = Field(
        default=None,
        ge=1,
        description="Minibatch size for PPO updates (None = auto)",
    )

    # Vector env parameters
    vector_num_envs: Optional[int] = Field(
        default=None,
        ge=1,
        description="Number of vectorized environments (None = auto)",
    )
    vector_num_workers: Optional[int] = Field(
        default=None,
        ge=1,
        description="Number of worker processes (None = auto based on CPU cores)",
    )
    vector_batch_size: Optional[int] = Field(
        default=None,
        ge=1,
        description="Batch size for vectorized env (None = num_envs)",
    )

    # Nested configurations
    ppo: PPOConfig = Field(
        default_factory=PPOConfig,
        description="PPO hyperparameters",
    )
    policy: PolicyConfig = Field(
        default_factory=PolicyConfig,
        description="Policy network configuration",
    )
    env: EnvironmentConfig = Field(
        default_factory=EnvironmentConfig,
        description="Environment configuration",
    )


# Observation space constants
OBS_MIN_VALUE: int = 0
OBS_MAX_VALUE: int = 255
OBS_NORMALIZATION_FACTOR: float = 1.0 / OBS_MAX_VALUE

# CLI defaults
DEFAULT_ANSI_STEPS: int = 128
DEFAULT_PROFILE_STEPS: int = 512


__all__ = [
    "Config",
    "RewardConfig",
    "EnvironmentConfig",
    "PPOConfig",
    "PolicyConfig",
    "TrainingConfig",
    "OBS_MIN_VALUE",
    "OBS_MAX_VALUE",
    "OBS_NORMALIZATION_FACTOR",
    "DEFAULT_ANSI_STEPS",
    "DEFAULT_PROFILE_STEPS",
]
